@article{mubarak2022emojis,
	
	title={Emojis as Anchors to Detect Arabic Offensive Language and Hate Speech},
	
	author={Mubarak, Hamdy and Hassan, Sabit and Chowdhury, Shammur Absar},
	
	journal={arXiv preprint arXiv:2201.06723},
	
	year={2022}
	
}

@article{hate2022machine,
	title={Hate in the machine: anti-Black and anti-Muslim social media posts as predictors of offline racially and religiously aggravated crime.},
	author={Williams, M. L., Burnap, P., Javed, A., Liu, H., and Ozalp},
	journal={The British Journal of Criminology 60(1): 93â€“117.},
	year={2020}
}


@inproceedings{abdul-mageed-etal-2021-arbert,
	title = {"{ARBERT} {\&} {MARBERT}: Deep Bidirectional Transformers for {A}rabic"},
	author = {"Abdul-Mageed, Muhammad  and
	Elmadany, AbdelRahim  and
	Nagoudi, El Moatez Billah"},
	booktitle = {"Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)"},
	month = {aug},
	year = {"2021"},
	address = {"Online"},
	publisher = {"Association for Computational Linguistics"},
	url = {"https://aclanthology.org/2021.acl-long.551"},
	doi = {"10.18653/v1/2021.acl-long.551"},
	pages ={ "7088--7105"},
	abstract = {"Pre-trained language models (LMs) are currently integral to many natural language processing systems. Although multilingual LMs were also introduced to serve many languages, these have limitations such as being costly at inference time and the size and diversity of non-English data involved in their pre-training. We remedy these issues for a collection of diverse Arabic varieties by introducing two powerful deep bidirectional transformer-based models, ARBERT and MARBERT. To evaluate our models, we also introduce ARLUE, a new benchmark for multi-dialectal Arabic language understanding evaluation. ARLUE is built using 42 datasets targeting six different task clusters, allowing us to offer a series of standardized experiments under rich conditions. When fine-tuned on ARLUE, our models collectively achieve new state-of-the-art results across the majority of tasks (37 out of 48 classification tasks, on the 42 datasets). Our best model acquires the highest ARLUE score (77.40) across all six task clusters, outperforming all other models including XLM-R Large ( 3.4x larger size). Our models are publicly available at https://github.com/UBC-NLP/marbert and ARLUE will be released through the same repository."}
}

@misc{ma2019nlpaug,
	title={NLP Augmentation},
	author={Edward Ma},
	howpublished={https://github.com/makcedward/nlpaug},
	year={2019}
}

@article{yang2019deepening,
	title={Deepening Hidden Representations from Pre-trained Language Models},
	author={Yang, Junjie and Zhao, Hai},
	journal={arXiv preprint arXiv:1911.01940},
	year={2019}
}

@article{devlin2018bert,
	title={Bert: Pre-training of deep bidirectional transformers for language understanding},
	author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	journal={arXiv preprint arXiv:1810.04805},
	year={2018}
}

@article{abdul2020arbert,
	title={ARBERT \& MARBERT: deep bidirectional transformers for Arabic},
	author={Abdul-Mageed, Muhammad and Elmadany, AbdelRahim and Nagoudi, El Moatez Billah},
	journal={arXiv preprint arXiv:2101.01785},
	year={2020}
}

@article{vaswani2017attention,
	title={Attention is all you need},
	author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
	journal={Advances in neural information processing systems},
	volume={30},
	year={2017}
}

@article{radford2018improving,
	title={Improving language understanding by generative pre-training},
	author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	year={2018}
}

@inproceedings{zhuang2021robustly,
	title={A Robustly Optimized BERT Pre-training Approach with Post-training},
	author={Zhuang, Liu and Wayne, Lin and Ya, Shi and Jun, Zhao},
	booktitle={Proceedings of the 20th Chinese National Conference on Computational Linguistics},
	pages={1218--1227},
	year={2021}
}

@inproceedings{farha2021benchmarking,
	title={Benchmarking transformer-based language models for Arabic sentiment and sarcasm detection},
	author={Farha, Ibrahim Abu and Magdy, Walid},
	booktitle={Proceedings of the sixth Arabic natural language processing workshop},
	pages={21--31},
	year={2021}
}


@inproceedings{aldjanabi2021arabic,
	title={Arabic Offensive and Hate Speech Detection Using a Cross-Corpora Multi-Task Learning Model},
	author={Aldjanabi, Wassen and Dahou, Abdelghani and Al-qaness, Mohammed AA and Abd Elaziz, Mohamed and Helmi, Ahmed Mohamed and Dama{\v{s}}evi{\v{c}}ius, Robertas},
	booktitle={Informatics},
	volume={8},
	number={4},
	pages={69},
	year={2021},
	organization={Multidisciplinary Digital Publishing Institute}
}

@inproceedings{abuzayed2020quick,
	title={Quick and simple approach for detecting hate speech in Arabic tweets},
	author={Abuzayed, Abeer and Elsayed, Tamer},
	booktitle={Proceedings of the 4th workshop on open-source Arabic Corpora and processing tools, with a shared task on offensive language detection},
	pages={109--114},
	year={2020}
}


@inproceedings{alshaalan2020hate,
	title={Hate speech detection in saudi twittersphere: A deep learning approach},
	author={Alshaalan, Raghad and Al-Khalifa, Hend},
	booktitle={Proceedings of the Fifth Arabic Natural Language Processing Workshop},
	pages={12--23},
	year={2020}
}

@inproceedings{faris2020hate,
	title={Hate Speech Detection using Word Embedding and Deep Learning in the Arabic Language Context.},
	author={Faris, Hossam and Aljarah, Ibrahim and Habib, Maria and Castillo, Pedro A},
	booktitle={ICPRAM},
	pages={453--460},
	year={2020}
}

@article{al2020hate,
	title={Hate Speech Classification in Arabic Tweets},
	author={Al-khalifa, Shaima and Aljarah, Ibrahim and Abushariah, Mohammad A M},
	journal={Journal of Theoretical and Applied Information Technology},
	volume={98},
	pages={1816--1831},
	year={2020}
}
